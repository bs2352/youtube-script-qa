# pyright: reportMissingModuleSource=false

import os
# from llama_index import download_loader, GPTVectorStoreIndex, Document
# from youtube_transcript_api import YouTubeTranscriptApi
import openai
import dotenv
import sys


DEFAULT_VID = "cEynsEWpXdA"
dotenv.load_dotenv()

# INDEX_STORAGE_DIR = "./indexes"
# if not os.path.isdir(INDEX_STORAGE_DIR):
#     os.mkdir(INDEX_STORAGE_DIR)


# dotenv.load_dotenv()
# OPENAI_API_KEY = os.environ["OPENAI_API_KEY"]
# os.environ["OPENAI_API_KEY"] = OPENAI_API_KEY
# openai.api_key = OPENAI_API_KEY


# video_id = "Tia4YJkNlQ0"
# video_id = "cEynsEWpXdA"

def get_transcription ():
    from youtube_transcript_api import YouTubeTranscriptApi

    # YoutubeTranscriptReader = download_loader("YoutubeTranscriptReader")
    # loader = YoutubeTranscriptReader()
    # documents = loader.load_data(ytlinks=["https://www.youtube.com/watch?v=cEynsEWpXdA"], languages=["ja"]) # MS2
    # documents = loader.load_data(ytlinks=["https://www.youtube.com/watch?v=tFgqdHKsOME"], languages=["ja"]) # MS
    # documents = loader.load_data(ytlinks=["https://www.youtube.com/watch?v=Tia4YJkNlQ0"], languages=["ja"]) # è¥¿åœ’å¯º
    # documents = loader.load_data(ytlinks=["https://www.youtube.com/watch?v=oc6RV5c1yd0"])
    # documents = loader.load_data(ytlinks=["https://www.youtube.com/watch?v=XJRoDEctAwA"])
    # for doc in documents:
    #     print(doc.text, '-------------------')

    MAXLENGTH = 500
    vid = DEFAULT_VID
    if len(sys.argv) > 1:
        vid = sys.argv[1]
    scripts = YouTubeTranscriptApi.get_transcript(video_id=vid, languages=["ja", "en", "en-US"])
    # text = ""
    for script in scripts:
        print(script)
        # text += script["text"].replace("\n", " ")
        # text += script["text"]
    # print(text)

# from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter
# from langchain.docstore.document import Document as LangChainDocument

# text_splitter = CharacterTextSplitter(
#     separator="\n",
#     chunk_size=300,
#     chunk_overlap=20
# )
# text_splitter = RecursiveCharacterTextSplitter(
#     chunk_size=500,
#     chunk_overlap=20,
# )
# texts = text_splitter.split_text(text)
# for t in texts:
#     print(t, len(t))
#     print('----')
# sys.exit()

# docs = [LangChainDocument(page_content=t) for t in texts]
# docs = [LangChainDocument(page_content=t) for i, t in enumerate(texts) if i < 3]


# from langchain.chains.summarize import load_summarize_chain
# from langchain.llms import OpenAI
# from langchain.chat_models import ChatOpenAI
# from langchain.prompts import PromptTemplate

# map_prompt_template = """ä»¥ä¸‹ã®å†…å®¹ã‚’ç°¡æ½”ã«ã¾ã¨ã‚ã¦ãã ã•ã„ã€‚:


# "{text}"


# ç°¡æ½”ãªè¦ç´„:"""
# map_prompt = PromptTemplate(template=map_prompt_template, input_variables=["text"])
# combine_prompt = map_prompt


# chain_type = "map_reduce"
# llm = OpenAI(
#     client=None,
#     model="text-davinci-003",
#     temperature=0.0,
#     # verbose=True,
#     max_tokens=1024,
# )
# llm = ChatOpenAI(
#     client=None,
#     model="gpt-3.5-turbo-16k",
#     temperature=0.0,
#     max_tokens=1024,
#     verbose=True,
# )
# chain = load_summarize_chain(
#     llm=llm,
#     chain_type=chain_type,
#     map_prompt=map_prompt,
#     combine_prompt=combine_prompt,
#     verbose=True
# )
# summarized_text = chain.run(docs)
# print(summarized_text)

# from langchain.embeddings import OpenAIEmbeddings
# from openai.embeddings_utils import cosine_similarity

# embeddings = OpenAIEmbeddings()
# doc_embeddings = embeddings.embed_documents(texts)

# prev_text = ""
# prev_embedding = [0.0e-10] * len(doc_embeddings[0])
# for text, embedding in zip(texts, doc_embeddings):
#     similarity = cosine_similarity(prev_embedding, embedding)
#     # print(text[:50], similarity, "###" if similarity < 0.80 else "")
#     if similarity < 0.80:
#         print(prev_text)
#         print(f'[{similarity}]------------')
#         print(text)
#         print('========')
#     prev_text = text
#     prev_embedding = embedding

def divide_topic ():
    from youtube_transcript_api import YouTubeTranscriptApi
    from langchain.prompts import PromptTemplate
    from langchain.chains import LLMChain
    from yts.utils import divide_transcriptions_into_chunks, setup_llm_from_environment

    transcriptions = YouTubeTranscriptApi.get_transcript(video_id=DEFAULT_VID, languages=["ja", "en"])
    chunks = divide_transcriptions_into_chunks(
        transcriptions=transcriptions,
        maxlength=200,
        overlap_length=0
    )
    # for chunk in chunks:
    #     print(chunk)

#     prompt_template = \
# """ä»¥ä¸‹ã«è¨˜è¼‰ã™ã‚‹æ–‡1ã¨æ–‡2ã¯Youtubeå‹•ç”»ã®ä¼šè©±ã®ä¸€éƒ¨ã‚’ä¸¦ã¹ãŸã‚‚ã®ã§ã€æ–‡2ã¯æ–‡1ã«ç¶šãä¼šè©±ã§ã™ã€‚
# æ–‡2ã¯æ–‡1ã¨åŒã˜è©±é¡Œã«ã¤ã„ã¦è¿°ã¹ã‚‰ã‚Œã¦ã„ã¾ã™ã‹ï¼Ÿ
# ãã‚Œã¨ã‚‚æ–‡2ã¯æ–‡1ã¨ã¯ç•°ãªã‚‹æ–°ãŸãªè©±é¡Œã«ã¤ã„ã¦è©±ã•ã‚Œã¦ã„ã¾ã™ã‹ï¼Ÿ
# æ–‡2ãŒæ–‡1ã¨åŒã˜è©±é¡Œã‚ã‚Œã°Yesã€ç•°ãªã‚‹è©±é¡Œã§ã‚ã‚Œã°Noã¨å›ç­”ã—ã¦ãã ã•ã„ã€‚
    prompt_template = \
"""ä»¥ä¸‹ã«è¨˜è¼‰ã™ã‚‹æ–‡1ã¨æ–‡2ã¯ä¼šè©±ã®ä¸€éƒ¨ã§ã™ã€‚æ–‡1ã«ç¶šãæ–‡2ã§è©±é¡Œã®å¤‰åŒ–ãŒã‚ã‚Šã¾ã™ã‹ï¼Ÿ
å¤‰åŒ–ãŒã‚ã‚Œã°ã§ã‚ã‚Œã°Yesã€ãªã‘ã‚Œã°Noã¨å›ç­”ã—ã¦ãã ã•ã„ã€‚

æ–‡1:
{text1}

æ–‡2:
{text2}

å›ç­”:
"""
    # print(prompt_template)
    prompt = PromptTemplate(template=prompt_template, input_variables=["text1", "text2"])

    llm_chain = LLMChain(
        llm = setup_llm_from_environment(),
        prompt=prompt
    )


    prev_text_1 = ""
    prev_text_2 = ""
    for idx, chunk in enumerate(chunks):
        if idx == 0:
            prev_text_1 = prev_text_2
            prev_text_2 = chunk.text
            continue
        inputs = {
            "text1": f'{prev_text_1.strip()}\n{prev_text_2.strip()}',
            "text2": chunk.text,
        }
        # print(prompt.format(**inputs))
        result = llm_chain.predict(**inputs).strip()
        if result == "Yes":
        # if result == "No":
            # print(prompt.format(**inputs))
            # input()
            print("#", end="", flush=True)
        else:
            print(".", end="", flush=True)
        prev_text_1 = prev_text_2
        prev_text_2 = chunk.text
        # break
        # input()
        # print(f".{result}", end="", flush=True)


def get_topic ():
    from youtube_transcript_api import YouTubeTranscriptApi
    from langchain.prompts import PromptTemplate
    from langchain.chains import LLMChain
    from yts.utils import divide_transcriptions_into_chunks, setup_llm_from_environment

    transcriptions = YouTubeTranscriptApi.get_transcript(video_id=DEFAULT_VID, languages=["ja", "en"])
    chunks = divide_transcriptions_into_chunks(
        transcriptions=transcriptions,
        maxlength=1000,
        overlap_length=5
    )
    # for chunk in chunks:
    #     print(chunk)

    prompt_template = \
"""ç§ã¯Youtubeå‹•ç”»ã®ã‚¢ã‚¸ã‚§ãƒ³ãƒ€ã‚’ä½œæˆã™ã‚‹ãŸã‚ã«å‹•ç”»å†…ã§è§¦ã‚Œã‚‰ã‚Œã¦ã„ã‚‹ãƒˆãƒ”ãƒƒã‚¯ã‚’ãƒªã‚¹ãƒˆã‚¢ãƒƒãƒ—ã—ã¦ã„ã¾ã™ã€‚
ã€Œæ–‡1ã€ã«ã¯ä¼šè©±ã®ä¸€éƒ¨ãŒè¨˜è¼‰ã•ã‚Œã¦ã„ã¾ã™ã€‚
ã€Œä»Šã¾ã§ã®ã¨ãƒ”ãƒƒã‚¯ã€ã«ã¯æ–‡1ã¾ã§ã«è§¦ã‚Œã‚‰ã‚Œã¦ã„ãŸãƒˆãƒ”ãƒƒã‚¯ã®ãƒªã‚¹ãƒˆãŒè¨˜è¼‰ã•ã‚Œã¦ã„ã¾ã™ã€‚
ã€Œæ–‡1ã€ã¨ã€Œä»Šã¾ã§ãƒˆãƒ”ãƒƒã‚¯ã€ã‹ã‚‰ã“ã®å‹•ç”»ã§è§¦ã‚Œã‚‰ã‚Œã¦ã„ã‚‹ãƒˆãƒ”ãƒƒã‚¯ã‚’æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚

ä»Šã¾ã§ã®ãƒˆãƒ”ãƒƒã‚¯:
{topics}

æ–‡1:
{text}

ãƒˆãƒ”ãƒƒã‚¯:
"""
    # print(prompt_template)
    prompt = PromptTemplate(template=prompt_template, input_variables=["topics", "text"])

    llm_chain = LLMChain(
        llm = setup_llm_from_environment(),
        prompt=prompt
    )

    topics = ""
    for idx, chunk in enumerate(chunks):
        input = {
            "topics": topics,
            "text": chunk.text,
        }
        print(prompt.format(**input))
        result = llm_chain.predict(**input)
        print(result)
        topics = result
        # break
        # input()
        # if idx > 5:
        #     break


def get_topic_from_summary ():
    from langchain.prompts import PromptTemplate
    from langchain.chains import LLMChain
    from yts.utils import setup_llm_from_environment
    import json
    import dotenv

    dotenv.load_dotenv()
    vid = DEFAULT_VID
    # vid = "Tia4YJkNlQ0"
    # vid = "Bd9LWW4cxEU"
    if len(sys.argv) >= 2:
        vid = sys.argv[1]
    with open(f"./{os.environ['SUMMARY_STORE_DIR']}/{vid}", "r") as f:
        summary = json.load(f)

    prompt_template = \
"""ç§ã¯Youtubeå‹•ç”»ã®ã‚¢ã‚¸ã‚§ãƒ³ãƒ€ã‚’ä½œæˆã—ã¦ã„ã¾ã™ã€‚
ä»¥ä¸‹ã«è¨˜è¼‰ã™ã‚‹å‹•ç”»ã®ã‚¿ã‚¤ãƒˆãƒ«ã¨è¦ç´„ã‹ã‚‰ã‚¢ã‚¸ã‚§ãƒ³ãƒ€ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚
ã‚¢ã‚¸ã‚§ãƒ³ãƒ€ã«ã¯å„ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã®ã‚¿ã‚¤ãƒˆãƒ«ã®ã¿ã‚’è¨˜è¼‰ã™ã‚‹ã‚ˆã†ã«ã—ã¦ãã ã•ã„ã€‚
ã¾ãŸã‚»ã‚¯ã‚·ãƒ§ãƒ³ã¯ã§ãã‚‹ã ã‘å°‘ãªãã‚·ãƒ³ãƒ—ãƒ«ã«ã¾ã¨ã‚ã¦ãã ã•ã„ã€‚

ã‚¿ã‚¤ãƒˆãƒ«:
{title}

è¦ç´„:
{summaries}

ã‚¢ã‚¸ã‚§ãƒ³ãƒ€:
"""

    # print(prompt_template)
    prompt = PromptTemplate(template=prompt_template, input_variables=["title", "summaries"])

    llm_chain = LLMChain(
        llm = setup_llm_from_environment(),
        prompt=prompt
    )

    summaries = ""
    for detail in summary["detail"]:
        summaries = f'{summaries}\nãƒ»{detail}'

    inputs = {
        "title": summary["title"],
        "summaries": summaries,
    }
    # print(prompt.format(**inputs))
    print(llm_chain.predict(**inputs))


def kmeans_embedding ():
    from youtube_transcript_api import YouTubeTranscriptApi
    import numpy as np
    from sklearn.cluster import KMeans
    from yts.utils import setup_embedding_from_environment, divide_transcriptions_into_chunks

    def _split_chunks (chunks, split_num = 5):
        total_time: float = chunks[-1].start + chunks[-1].duration
        delta: float = total_time // split_num
        splited_chunks = []
        for tc in chunks:
            idx = int(tc.start // delta)
            idx = idx if idx < split_num else split_num
            if idx + 1 > len(splited_chunks):
                splited_chunks.append([])
            splited_chunks[idx].append(tc)
        return splited_chunks

    def _make_init_cluster(splited_chunks, embeddings):
        init_cluster = []
        idx = 0
        for splited_chunk in splited_chunks:
            count = len(splited_chunk)
            cluster = []
            for i in range(idx, idx + count):
                cluster.append(embeddings[i])
            x_cluster_mean = np.mean(np.array(cluster), axis=0)
            init_cluster.append(x_cluster_mean)
            idx += count
        x_init_cluster = np.array(init_cluster)
        return x_init_cluster

    vid = DEFAULT_VID
    if len(sys.argv) >= 2:
        vid = sys.argv[1]
    transcriptions = YouTubeTranscriptApi.get_transcript(video_id=vid, languages=["ja", "en"])
    chunks = divide_transcriptions_into_chunks(
        transcriptions=transcriptions,
        maxlength=300,
        overlap_length=3,
    )
    texts = []
    for chunk in chunks:
        texts.append(chunk.text)

    llm_embedding = setup_embedding_from_environment()
    embeddings = llm_embedding.embed_documents(texts)

    splited_chunks = _split_chunks(chunks)
    x_init_cluster = _make_init_cluster(splited_chunks, embeddings)
    # print(x_init_cluster, x_init_cluster.shape)

    x_train = np.array(embeddings)

    # kmeans = KMeans(n_clusters=5, random_state=0)
    kmeans = KMeans(n_clusters=5, random_state=0, init=x_init_cluster)
    predicted = kmeans.fit_predict(x_train)
    print(predicted)


def async_run ():
    from youtube_transcript_api import YouTubeTranscriptApi
    from yts.utils import setup_llm_from_environment, divide_transcriptions_into_chunks
    from langchain.prompts import PromptTemplate
    from langchain.schema import HumanMessage
    from langchain.chains import LLMChain
    import asyncio
    import json

    PROMPT_TEMPLATE = """ä»¥ä¸‹ã®å†…å®¹ã‚’200å­—ä»¥å†…ã®æ—¥æœ¬èªã§ç°¡æ½”ã«è¦ç´„ã—ã¦ãã ã•ã„ã€‚:


"{text}"


ç°¡æ½”ãªè¦ç´„:"""

    PROMPT_TEMPLATE = """ä»¥ä¸‹ã®å†…å®¹ã‚’é‡è¦ãªæƒ…å ±ã¯ã§ãã‚‹ã ã‘æ®‹ã—ã¦è¦ç´„ã—ã¦ãã ã•ã„ã€‚:


"{text}"


è¦ç´„:"""

    # async def async_generate (llm, message):
    #     results = await llm.agenerate([message])
    #     return results.generations[0][0].text

    # async def generate_concurrently (chunks):
    #     llm = setup_llm_from_environment()
    #     prompt_template = PromptTemplate(template=PROMPT_TEMPLATE, input_variables=["text"])
    #     tasks = []
    #     for chunk in chunks:
    #         message = [
    #             HumanMessage(content=prompt_template.format(text=chunk.text))
    #         ]
    #         tasks.append(async_generate(llm, message))
    #     results = await asyncio.gather(*tasks)

    #     for chunk, result in zip(chunks, results):
    #         print("========\n", chunk.text, "\n------\n", result)
    #     print(len(chunks), len(results))

    async def async_generate (chain, chunk):
        answer = await chain.arun(text=chunk)
        return answer

    async def generate_concurrently (chunks):
        llm = setup_llm_from_environment()
        prompt_template = PromptTemplate(template=PROMPT_TEMPLATE, input_variables=["text"])
        chain = LLMChain(llm=llm, prompt=prompt_template)
        tasks = [async_generate(chain, chunk) for chunk in chunks]
        results = await asyncio.gather(*tasks)

        for chunk, result in zip(chunks, results):
            print("========\n", chunk.text, "\n------\n", result)
        print(len(chunks), len(results))

    def generate_concurrently_2 (chunks):
        llm = setup_llm_from_environment()
        prompt_template = PromptTemplate(template=PROMPT_TEMPLATE, input_variables=["text"])
        chain = LLMChain(llm=llm, prompt=prompt_template)
        tasks = [async_generate(chain, chunk) for chunk in chunks]
        gather = asyncio.gather(*tasks)
        loop = asyncio.get_event_loop()
        results = loop.run_until_complete(gather)
        return results


    vid = DEFAULT_VID
    if len(sys.argv) >= 2:
        vid = sys.argv[1]
    transcriptions = YouTubeTranscriptApi.get_transcript(video_id=vid, languages=["ja", "en"])
    chunks = divide_transcriptions_into_chunks(
        transcriptions=transcriptions,
        maxlength=1000,
        overlap_length=5,
    )
    # asyncio.run(generate_concurrently(chunks))
    results = generate_concurrently_2(chunks)
    for chunk, result in zip(chunks, results):
        print("========\n", chunk.text, "\n------\n", result)
    print(len(chunks), len(results))


def count_tokens ():
    import tiktoken
    from yts.utils import count_tokens

    text = "ä»Šæ—¥ã¯ã‚ªãƒªãƒƒã‚¯ã‚¹vsæ—¥æœ¬ãƒãƒ ã§ã™ã€‚æœ€å¾Œã ã—è¡ŒããŸã‹ã£ãŸãªã€‚"
    models = [
        "text-davinci-003",
        "gpt-3.5-turbo",
        "gpt-3.5-turbo-0613",
        "gpt-3.5-turbo-16k",
        "gpt-35-turbo",
        "gpt-35-turbo-0613",
        "gpt-35-turbo-16k",
        "gpt-4-0314",
        "gpt-4-32k-0314",
        "gpt-4-0613",
        "gpt-4-32k-0613",
    ]
    for model in models:
        try:
            model_x = model.replace("35", "3.5")
            encoding = tiktoken.encoding_for_model(model_x)
        except:
            print("Warning: model not found. Using cl100k_base encoding.")
            encoding = tiktoken.get_encoding("cl100k_base")
        # ã¨ã‚Šã‚ãˆãšã ã„ãŸã„ã§OK
        # https://cookbook.openai.com/examples/how_to_count_tokens_with_tiktoken
        count = len(encoding.encode(text))
        print(model, ":", count, "/", count_tokens(text))


from tenacity import retry, wait_fixed, stop_after_attempt, before_log
import logging
# logging.basicConfig(stream=sys.stderr, level=logging.DEBUG)
logger = logging.getLogger(__name__)
@retry(
    stop=stop_after_attempt(5),
    wait=wait_fixed(3),
    before=before_log(logger, logging.DEBUG)
)
def test_function_calling ():
    import openai
    import json
    import asyncio

    functions = [
        {
            # "name": "answer_from_local_information",
            "name": "answer_question_about_specific_things",
            # "description": "æŒ‡å®šã•ã‚ŒãŸå‹•ç”»å†…ã§è³ªå•ã«é–¢é€£ã™ã‚‹å±€æ‰€çš„ãªæƒ…å ±ã®ã¿ã‚’åˆ©ç”¨ã—ã¦å›ç­”ã‚’ç”Ÿæˆã™ã‚‹",
            # "descriotion": "Generate answers using only local information relevant to the question within the specified video",
            # "description": "æŒ‡å®šã•ã‚ŒãŸå‹•ç”»ã§è§¦ã‚Œã‚‰ã‚Œã¦ã„ã‚‹ç‰¹å®šã®äº‹æŸ„ã«é–¢ã™ã‚‹è³ªå•ã«å›ç­”ã™ã‚‹",
            "description": "Answer questions about specific things mentioned in a given video",
            "parameters": {
                "type": "object",
                "properties": {
                    "title": {
                        "type": "string",
                        "description": "å‹•ç”»ã®ã‚¿ã‚¤ãƒˆãƒ«"
                    },
                    "question": {
                        "type": "string",
                        "description": "è³ªå•",
                    }
                },
                "required": ["title", "question"]
            }
        },
        {
            # "name": "answer_from_all_information",
            "name": "answer_question_about_general_content",
            # "description": "æŒ‡å®šã•ã‚ŒãŸå‹•ç”»å†…ã®å…¨ã¦ã®æƒ…å ±ã‚’åˆ©ç”¨ã—ã¦è³ªå•ã«å¯¾ã™ã‚‹å›ç­”ã‚’ç”Ÿæˆã™ã‚‹",
            # "description": "Generates an answer to a question using all the information in the specified video",
            # "description": "æŒ‡å®šã•ã‚ŒãŸå‹•ç”»ã®å†…å®¹å…¨èˆ¬ã«é–¢ã™ã‚‹è³ªå•ã«å›ç­”ã™ã‚‹",
            # "description": "Answer questions about the general content of a given video",
            "description": "View the entire video and Answer questions about the general content of a given video",
            "parameters": {
                "type": "object",
                "properties": {
                    "title": {
                        "type": "string",
                        "description": "å‹•ç”»ã®ã‚¿ã‚¤ãƒˆãƒ«",
                    },
                    "question": {
                        "type": "string",
                        "description": "è³ªå•",
                    }
                },
                "required": ["title", "question"]
            }
        },
        {
            "name": "summarize",
            "description": "summarize the content of a given video",
            "parameters": {
                "type": "object",
                "properties": {
                    "title": {
                        "type": "string",
                        "description": "å‹•ç”»ã®ã‚¿ã‚¤ãƒˆãƒ«",
                    }
                },
                "required": ["title"],
            }
        },
    ]

    question_prefix = "ã€ŒAzure OpenAI Developers ã‚»ãƒŸãƒŠãƒ¼ç¬¬ 2 å›ã€ã¨ã„ã†ã‚¿ã‚¤ãƒˆãƒ«ã®å‹•ç”»ã‹ã‚‰æ¬¡ã®è³ªå•ã«å›ç­”ã—ã¦ãã ã•ã„ã€‚"
    # question_prefix = "ã€Œé‚ªé¦¬å°å›½ã¯ã©ã“ã«ã‚ã£ãŸï¼Ÿã€ã¨ã„ã†ã‚¿ã‚¤ãƒˆãƒ«ã®å‹•ç”»ã‹ã‚‰è³ªå•ã«å›ç­”ã—ã¦ãã ã•ã„ã€‚"
    questions = [
        ("ã“ã®å‹•ç”»ã§ã¯ã©ã®ã‚ˆã†ãªãƒˆãƒ”ãƒƒã‚¯ã«ã¤ã„ã¦è©±ã•ã‚Œã¦ã„ã¾ã™ã‹ï¼Ÿ", 1),
        ("ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã¨ã¯ä½•ã§ã™ã‹ï¼Ÿ", 0),
        ("ã©ã®ã‚ˆã†ãªè©±é¡ŒãŒã‚ã‚Šã¾ã™ã‹ï¼Ÿ", 1),
        ("å‹•ç”»ã®å†…å®¹ã‚’ç°¡å˜ã«æ•™ãˆã¦", 1),
        ("ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã€ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢ã€ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯æ¤œç´¢ã®é•ã„ã‚’æ•™ãˆã¦ãã ã•ã„ã€‚", 0),
        ("ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«æ‰‹é †ã‚’æ•™ãˆã¦", 0),
        ("ã“ã®å‹•ç”»ã§è©±ã—ã¦ã„ã‚‹ã®ã¯äººã‚’å…¨ã¦æ•™ãˆã¦ãã ã•ã„ã€‚", 1),
        ("ã“ã®å‹•ç”»ã§è©±ã—ã¦ã„ã‚‹äººã¯èª°ã§ã™ã‹ï¼Ÿ", 0),
        ("ã“ã®å‹•ç”»ã«ç™»å ´ã™ã‚‹äººç‰©ã‚’å…¨ã¦æ•™ãˆã¦ãã ã•ã„ã€‚", 1),
        ("é‚ªé¦¬å°å›½ã®å€™è£œåœ°ã‚’å…¨ã¦æ•™ãˆã¦ãã ã•ã„ã€‚", 1),
        ("é‚ªé¦¬å°å›½ã®å€™è£œåœ°ã¯ï¼Ÿ", 0),
        ("é‚ªé¦¬å°å›½ã®å€™è£œåœ°ã¯ï¼Ÿå…¨ã¦ç­”ãˆã¦ãã ã•ã„ã€‚", 1),
        ("å‹•ç”»å†…ã§ç´¹ä»‹ã•ã‚Œã¦ã„ã‚‹é‚ªé¦¬å°å›½ã®å€™è£œåœ°ã‚’å…¨ã¦ç­”ãˆã¦ãã ã•ã„ã€‚", 1),
        ("ä¸€èˆ¬çš„ã«é‚ªé¦¬å°å›½ã¯ã©ã“ã«ã‚ã£ãŸã¨è¨€ã‚ã‚Œã¦ã„ã¾ã™ã‹ï¼Ÿ", 0),
        ("å‹•ç”»å†…ã§ç´¹ä»‹ã•ã‚Œã¦ã„ã‚‹é‚ªé¦¬å°å›½ã®æ‰€åœ¨åœ°ã‚’å…¨ã¦ç­”ãˆã¦ãã ã•ã„ã€‚", 1),
        ("Please tell me all the candidate locations for Yamataikoku.", 1),
        ("é‚ªé¦¬å°å›½ã¯ã©ã“ã«ã‚ã£ãŸã®ã§ã™ã‹ï¼Ÿè€ƒãˆã‚‰ã‚Œã‚‹åœ°åŸŸã‚’å…¨ã¦ç­”ãˆã¦ãã ã•ã„", 1),
    ]

    openai.api_key = os.environ['OPENAI_API_KEY']
    tasks = [
        openai.ChatCompletion.acreate(
            model='gpt-3.5-turbo-0613',
            messages=[
                {'role': 'user', 'content': f'{question_prefix}\n\nè³ªå•ï¼š{question[0]}\n'}
            ],
            functions=functions,
            function_call="auto",
            temperature=0.3,
        )
        for question in questions
    ]
    gather = asyncio.gather(*tasks)
    loop = asyncio.get_event_loop()
    results = loop.run_until_complete(gather)

    for question, result in zip(questions, results):
        message = result["choices"][0]["message"]
        answer = message["function_call"]["name"] if "function_call" in message else "none"
        judge = "OK" if answer == functions[question[1]]["name"] else "NG"
        print(f'{judge}\t{question[0]}\t{answer}')

    # message = completion['choices'][0]["message"]
    # if message["function_call"]:
    #     print("function", message["function_call"]["name"])
    #     print("function", json.loads(message["function_call"]["arguments"]))
    # if message["content"]:
    #     print("content", message["content"])


def qa_with_function_calling ():
    from pytube import YouTube
    from langchain.prompts import PromptTemplate
    from langchain.chains import LLMChain
    from langchain.schema import LLMResult, ChatGeneration
    from yts.utils import setup_llm_from_environment

    vid = DEFAULT_VID
    if len(sys.argv) >= 2:
        vid = sys.argv[1]
    url = f'https://www.youtube.com/watch?v={vid}'
    title = YouTube(url).vid_info["videoDetails"]["title"]

    def is_question_about_local (question):
        functions = [
            {
                "name": "answer_question_about_specific_things",
                "description": "Answer questions about specific things mentioned in a given video",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "title": {
                            "type": "string",
                            "description": "å‹•ç”»ã®ã‚¿ã‚¤ãƒˆãƒ«"
                        },
                        "question": {
                            "type": "string",
                            "description": "è³ªå•",
                        }
                    },
                    "required": ["title", "question"]
                }
            },
            {
                "name": "answer_question_about_general_content",
                "description": "View the entire video and Answer questions about the general content of a given video",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "title": {
                            "type": "string",
                            "description": "å‹•ç”»ã®ã‚¿ã‚¤ãƒˆãƒ«",
                        },
                        "question": {
                            "type": "string",
                            "description": "è³ªå•",
                        }
                    },
                    "required": ["title", "question"]
                }
            },
        ]
        # question_prefix = f"ã€Œ{title}ã€ã¨ã„ã†ã‚¿ã‚¤ãƒˆãƒ«ã®å‹•ç”»ã‹ã‚‰è³ªå•ã«å›ç­”ã—ã¦ãã ã•ã„ã€‚"
        # completion = openai.ChatCompletion.create(
        #     model='gpt-3.5-turbo-0613',
        #     messages=[
        #         {'role': 'user', 'content': f'{question_prefix}\n{question}'}
        #     ],
        #     functions=functions,
        #     function_call="auto",
        #     temperature=0.3,
        # )
        # message = completion["choices"][0]["message"] # type: ignore

        # See langchain/chains/openai_functions/openai.py
        llm = setup_llm_from_environment()
        prompt = PromptTemplate(
            template= "ã€Œ{title}ã€ã¨ã„ã†ã‚¿ã‚¤ãƒˆãƒ«ã®å‹•ç”»ã‹ã‚‰æ¬¡ã®è³ªå•ã«å›ç­”ã—ã¦ãã ã•ã„ã€‚\n\nè³ªå•ï¼š{question}\n",
            input_variables=["title", "question"]
        )
        chain = LLMChain(
            llm=llm,
            prompt=prompt,
            llm_kwargs={
                "functions": functions
            },
            # output_parser=JsonOutputFunctionsParser(args_only=False),
            output_key="function",
            verbose=True
        )
        result: LLMResult = chain.generate([{"title":title, "question":question}])
        generation: ChatGeneration = result.generations[0][0] # type: ignore
        message = generation.message.additional_kwargs
        # print(generation)
        # print(generation.message.additional_kwargs["function_call"]["name"])
        # print(message)
        # sys.exit(0)

        func_name = functions[0]["name"]
        if "function_call" in message:
            func_name = message["function_call"]["name"]
        return True if func_name == functions[0]["name"] else False


    def answer_from_search (question):
        from yts.qa import YoutubeQA
        yqa = YoutubeQA(vid)
        return yqa.run(question)


    def answer_from_summary (question):
        import json
        summary_file = f'{os.environ["SUMMARY_STORE_DIR"]}/{vid}'
        if not os.path.exists(summary_file):
            from yts.summarize import YoutubeSummarize, MODE_DETAIL
            ys = YoutubeSummarize(vid)
            summary = ys.run(mode=MODE_DETAIL)
        else:
            with open(summary_file, 'r') as f:
                summary = json.load(f)
        summary_detail = "\n".join(summary["detail"]) # type: ignore

        llm = setup_llm_from_environment()
        prompt = PromptTemplate(
            template= "'{question}\nä»¥ä¸‹ã®æ–‡æ›¸ã®å†…å®¹ã‹ã‚‰å›ç­”ã—ã¦ãã ã•ã„ã€‚\n\n{summary_detail}\n",
            input_variables=["question", "summary_detail"]
        )
        chain = LLMChain(
            llm=llm,
            prompt=prompt,
            # verbose=True
        )
        result = chain.run(question=question, summary_detail=summary_detail)

        return result


    question = input("Query: ").strip()
    try:
        is_about_local = is_question_about_local(question)
    except:
        is_about_local = True
    # is_about_local = False
    if is_about_local:
        answer = answer_from_search(question)
        print("# local\n", f'{answer}\n')
    else:
        answer = answer_from_summary(question)
        print("# global\n", answer)


def test_loading ():
    import asyncio
    import time

    async def async_sleep ():
        sec = 0
        while sec <= 10:
            await asyncio.sleep(2)
            print(f"# {sec} sleep.")
            sec += 2
        await asyncio.sleep(2)

    async def loading ():
        chars = ['/', '-', '\\', '|', '/', '-', '\\', '|', 'ğŸ˜', 'ğŸ˜']
        # chars = ['!', '#', '$', '%', '&', '/', '-', '\\', '-', '+']
        i = 0
        while i >= 0:
            i %= len(chars)
            sys.stdout.write("\033[2K\033[G %s " % chars[i])
            sys.stdout.flush()
            await asyncio.sleep(1)
            # sys.stdout.flush()
            i += 1

    x = asyncio.ensure_future(loading())
    print(x)

    tasks = [async_sleep()]
    gather = asyncio.gather(*tasks)
    loop = asyncio.get_event_loop()
    result = loop.run_until_complete(gather)
    # loading()

    x.cancel()
    sys.stdout.write("\033[2K\033[G")
    sys.stdout.flush()


import asyncio
from yts.utils import loading_for_async_func
@loading_for_async_func
def test_decorate_loading ():
    async def func ():
        await asyncio.sleep(10)
    tasks = [func()]
    gather = asyncio.gather(*tasks)
    loop = asyncio.get_event_loop()
    loop.run_until_complete(gather)[0]
    return "fin"


def embedding_async ():
    from langchain.embeddings import OpenAIEmbeddings
    from llama_index import GPTVectorStoreIndex, Document, ServiceContext, LLMPredictor, LangchainEmbedding
    from yts.utils import setup_llm_from_environment, setup_embedding_from_environment
    import dotenv

    dotenv.load_dotenv(".env")

    texts = [
        "ã‚ªãƒªãƒƒã‚¯ã‚¹ãƒãƒ•ã‚¡ãƒ­ãƒ¼ã‚ºå„ªå‹ã™ã‚‹ãï¼" for _ in range(0, 100)
    ]
    llm_embeddings = setup_embedding_from_environment()

    # OK
    # embedding = llm_embeddings.embed_query("ã‚ªãƒªãƒƒã‚¯ã‚¹ãƒãƒ•ã‚¡ãƒ­ãƒ¼ã‚ºå„ªå‹ã™ã‚‹ãï¼")
    # embedding = asyncio.run(llm_embeddings.aembed_query("ã‚ªãƒªãƒƒã‚¯ã‚¹ãƒãƒ•ã‚¡ãƒ­ãƒ¼ã‚ºå„ªå‹ã™ã‚‹ãï¼"))

    # OK
    # async def aget_embedding ():
    #     tasks = [llm_embeddings.aembed_query("ã‚ªãƒªãƒƒã‚¯ã‚¹ãƒãƒ•ã‚¡ãƒ­ãƒ¼ã‚ºå„ªå‹ã™ã‚‹ãï¼")]
    #     return await asyncio.gather(*tasks)
    # embedding = asyncio.run(aget_embedding())

    # OK
    # embedding = llm_embeddings.embed_documents(texts)
    # async def aembed_documents ():
    #     return await llm_embeddings.aembed_documents(texts)
    # embedding = asyncio.run(aembed_documents())

    # NG
    async def aget_embedding_2 ():
        tasks = [llm_embeddings.aembed_query(text) for text in texts]
        return await asyncio.gather(*tasks)
    embedding = asyncio.run(aget_embedding_2())

    print(embedding)

    # texts = [
    #     "ã‚ªãƒªãƒƒã‚¯ã‚¹ãƒãƒ•ã‚¡ãƒ­ãƒ¼ã‚ºå„ªå‹ã™ã‚‹ãï¼" for _ in range(0, 50)
    #     # "ãƒ“ãƒƒã‚°ãƒœã‚¹ã‚‚é ‘å¼µã£ã¦æ¬²ã—ã„ã€‚"
    # ]
    # documents = [
    #     Document(text=text.replace("\n", " "), doc_id=f"id-{idx}") for idx, text in enumerate(texts)
    # ]
    # dotenv.load_dotenv(".env")
    # llm = setup_llm_from_environment()
    # embedding = setup_embedding_from_environment()
    # llm_predictor: LLMPredictor = LLMPredictor(llm=llm)
    # embedding_llm: LangchainEmbedding = LangchainEmbedding(embedding)
    # service_context: ServiceContext = ServiceContext.from_defaults(
    #     llm_predictor = llm_predictor,
    #     embed_model = embedding_llm,
    # )
    # index: GPTVectorStoreIndex = GPTVectorStoreIndex.from_documents(
    #     documents,
    #     service_context=service_context,
    #     show_progress=True,
    #     use_async=True, # ãƒã‚°ï¼Ÿãªãœã‹ã‚¨ãƒ©ãƒ¼ã«ãªã‚‹ã€‚
    # )
    print("fin")

if __name__ == "__main__":
    # get_transcription()
    # divide_topic()
    # get_topic()
    get_topic_from_summary()
    # kmeans_embedding()
    # async_run()
    # count_tokens()
    # test_function_calling()
    # qa_with_function_calling()
    # test_loading()
    # print(test_decorate_loading())
    # embedding_async()